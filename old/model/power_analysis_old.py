# -*- coding: utf-8 -*-
"""
Created on Thu Apr  8 09:16:24 2021

@author: de_hauk
"""


from tqdm import tqdm
import pandas as pd
import numpy as np
from functools import partial
from scipy import optimize,special
np.random.seed(1993)
import numpy as np
import pandas as pd
import glob
import random 
from joblib import Parallel, delayed
import os
pathtomodels = r'C:\Users\de_hauk\Documents\GitHub\Apps_Tzakiris_2013\Comp_modeling\model'

os.chdir(pathtomodels)

from model_functions import VIEW_INDIPENDENTxCONTEXT,VIEW_DEPENDENT,VIEW_DEPENDENTxCONTEXT_DEPENDENT
from model_functions import VIEW_INDEPENDENT, VIEW_INDEPENDENTxVIEW_DEPENDENT
from model_functions import VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT,bic

from class_import_power_analysis import power_analyses

def bic_data():
    params_M1_name = ['alpha', 'sigma', 'beta', 'lamd_a'] 
    params_M2_name = ['alpha', 'beta', 'lamd_a']
    params_M3_name = ['alpha', 'sigma', 'beta', 'lamd_a']
    params_M4_name = ['alpha', 'beta', 'lamd_a']
    params_M5_name = ['alpha_ind', 'alpha_dep', 'beta', 'lamd_a_ind', 'lamd_a_dep']
    params_M6_name = ['alpha_ind', 'alpha_dep', 'sigma', 'beta', 'lamd_a_ind', 'lamd_a_dep']

    parameter_N = {'VIEW_INDIPENDENTxCONTEXT': len(params_M1_name),
                    'VIEW_DEPENDENT': len(params_M2_name),
                    'VIEW_DEPENDENTxCONTEXT_DEPENDENT': len(params_M3_name),
                    'VIEW_INDEPENDENT': len(params_M4_name),
                    'VIEW_INDEPENDENTxVIEW_DEPENDENT': len(params_M5_name),
                    'VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT': len(params_M6_name)}   
    return parameter_N

    
def fit_data_base(data_vpn):
    ### data has to be list
    
    from model_functions import VIEW_INDIPENDENTxCONTEXT,VIEW_DEPENDENT,VIEW_DEPENDENTxCONTEXT_DEPENDENT
    from model_functions import VIEW_INDEPENDENT, VIEW_INDEPENDENTxVIEW_DEPENDENT
    from model_functions import VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT,bic

    import pandas as pd
    import numpy as np
    from functools import partial
    from scipy import optimize,special
    
    epsilon_param = .01
    x_0_bfgs = 0.5
    #func calls & rand starts
    vpn = data_vpn[0]
    curr_data_vpn = data_vpn
    # # get data numpy
    stim_IDs_VI=    curr_data_vpn[0:,2]  #stimulus IDs of winning model 
    new_ID=         curr_data_vpn[0:,1]      #trials where new ID is introduced 
    n_prev_pres=    curr_data_vpn[0:,4]    #number_of_prev_presentations
    stim_IDs_VD=    curr_data_vpn[0:,3]  #view dependent
    VPN_output =    curr_data_vpn[0:,0]     #VPN answers
    verbose =       False
    #data_All = [VPN_output, new_ID, numb_prev_presentations, stim_IDs_VI,stim_IDs_VD,verbose]

    data_ALL = [VPN_output.astype(int), 
                new_ID.astype(int), 
                n_prev_pres.astype(int), 
                stim_IDs_VI.astype(str), 
                stim_IDs_VD.astype(str), 
                verbose]     
    ##### Data

    params_M1_name = ['alpha', 'sigma', 'beta', 'lamd_a'] 
    params_M2_name = ['alpha', 'beta', 'lamd_a']
    params_M3_name = ['alpha', 'sigma', 'beta', 'lamd_a']
    params_M4_name = ['alpha', 'beta', 'lamd_a']
    params_M5_name = ['alpha_ind', 'alpha_dep', 'beta', 'lamd_a_ind', 'lamd_a_dep']
    params_M6_name = ['alpha_ind', 'alpha_dep', 'sigma', 'beta', 'lamd_a_ind', 'lamd_a_dep']

    models_names = ['VIEW_INDIPENDENTxCONTEXT',
                    'VIEW_DEPENDENT',
                    'VIEW_DEPENDENTxCONTEXT_DEPENDENT',
                    'VIEW_INDEPENDENT',
                    'VIEW_INDEPENDENTxVIEW_DEPENDENT',
                    'VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT',
                    'random_choice']
    models_names_bic = ['VIEW_INDIPENDENTxCONTEXT',
                        'VIEW_DEPENDENT',
                        'VIEW_DEPENDENTxCONTEXT_DEPENDENT',
                        'VIEW_INDEPENDENT',
                        'VIEW_INDEPENDENTxVIEW_DEPENDENT',
                        'VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT']
    
    parameter_est = {'VIEW_INDIPENDENTxCONTEXT': pd.DataFrame(index=params_M1_name),
                    'VIEW_DEPENDENT': pd.DataFrame(index=params_M2_name),
                    'VIEW_DEPENDENTxCONTEXT_DEPENDENT': pd.DataFrame(index=params_M3_name),
                    'VIEW_INDEPENDENT': pd.DataFrame(index=params_M4_name),
                    'VIEW_INDEPENDENTxVIEW_DEPENDENT': pd.DataFrame(index=params_M5_name),
                    'VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT': pd.DataFrame(index=params_M6_name)}   

    
    
    ########## Model Optim
    
    
    print('VIEW_INDIPENDENTxCONTEXT')

    bounds_M1 = [(0,1),(0,1),(.1,20),(0,2)]
    
    part_func_M1 = partial(VIEW_INDIPENDENTxCONTEXT,data_ALL,None) 
    res1 = optimize.fmin_l_bfgs_b(part_func_M1,
                                  approx_grad = True,
                                  bounds = bounds_M1, 
                                  x0 = [x_0_bfgs for i in range(len(bounds_M1))],
                                  epsilon=epsilon_param)
    #parameter_est['VIEW_INDIPENDENTxCONTEXT'][vpn] = res1[0]
    
    ##### VIEW_DEPENDENT
    print('VIEW_DEPENDENT')
            
    bounds_M2 = [(0,1),(.1,20),(0,2)]
    
    part_func_M2 = partial(VIEW_DEPENDENT,data_ALL,None) 
    res2 = optimize.fmin_l_bfgs_b(part_func_M2,
                                  approx_grad = True,
                                  bounds = bounds_M2, 
                                  x0 = [x_0_bfgs for i in range(len(bounds_M2))],
                                  epsilon=epsilon_param)
    
    #parameter_est['VIEW_DEPENDENT'][vpn] = res2[0]
    
    ##### VIEW_DEPENDENTxCONTEXT_DEPENDENT
    print('VIEW_DEPENDENTxCONTEXT_DEPENDENT')

    bounds_M3 = [(0,1),(0,1),(.1,20),(0,2)]

    part_func_M3 = partial(VIEW_DEPENDENTxCONTEXT_DEPENDENT,data_ALL,None) 
    res3 = optimize.fmin_l_bfgs_b(part_func_M3,
                                  approx_grad = True,
                                  bounds = bounds_M3, 
                                  x0 = [x_0_bfgs for i in range(len(bounds_M3))],
                                  epsilon=epsilon_param)
    #parameter_est['VIEW_DEPENDENTxCONTEXT_DEPENDENT'][vpn] = res3[0]
    
    ##### VIEW_INDEPENDENT
    print('VIEW_INDEPENDENT')

    bounds_M4 = [(0,1),(.1,20),(0,2)]

    part_func_M4 = partial(VIEW_INDEPENDENT,data_ALL,None) 
    res4 = optimize.fmin_l_bfgs_b(part_func_M4,
                                  approx_grad = True,
                                  bounds = bounds_M4, 
                                  x0 = [x_0_bfgs for i in range(len(bounds_M4))],
                                  epsilon=epsilon_param)
    #parameter_est['VIEW_INDEPENDENT'][vpn] = res4[0]
    
    ##### VIEW_INDEPENDENTxVIEW_DEPENDENT
    print('VIEW_INDEPENDENTxVIEW_DEPENDENT')
            
    bounds_M5 = [(0,1),(0,1),(.1,20),(0,2),(0,2)]

    part_func_M5 = partial(VIEW_INDEPENDENTxVIEW_DEPENDENT,data_ALL,None) 
    res5 = optimize.fmin_l_bfgs_b(part_func_M5,
                                  approx_grad = True,
                                  bounds = bounds_M5, 
                                  x0 = [x_0_bfgs for i in range(len(bounds_M5))],
                                  epsilon=epsilon_param)
    #parameter_est['VIEW_INDEPENDENTxVIEW_DEPENDENT'][vpn] = res5[0]
    
    ##### VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT
    print('VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT')
    #params_M6_name = ['alpha_ind', 'alpha_dep', 'sigma', 'beta', 'lamd_a_ind', 'lamd_a_dep']

    bounds_M6 = [(0,1),(0,1),(0,1),(.1,20),(0,2),(0,2)]

    part_func_M6 = partial(VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT,data_ALL,None) 
    res6 = optimize.fmin_l_bfgs_b(part_func_M6,
                                  approx_grad = True,
                                  bounds = bounds_M6, 
                                  x0 = [x_0_bfgs for i in range(len(bounds_M6))],
                                  epsilon=epsilon_param)
    
    #parameter_est['VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT'][vpn] = res6[0]
    
    
    ##### RND Choice model
    rnd_choice = np.sum([np.log(0.5) for i in range(len(VPN_output))])
    
    re_evidence_subj = [(-1*i[1]) for i in [res1,res2,res3,res4,res5,res6]] + [rnd_choice]
    
    res_ev_DF = pd.DataFrame(index = models_names)
    res_ev_DF['ev'] = re_evidence_subj
    ### Subject BF_LOG
    bf_log_subj = re_evidence_subj[0]-special.logsumexp(np.array(re_evidence_subj))
    
    return res_ev_DF


def ttest_procedure(subject_level_model_evidence):
    import pandas as pd
    import pingouin as pg
    import numpy as np
    ''' 
    As described in Apps & Tsakiris, we ran a series of independent
    two-sided t-tests and corrected via Benjaminiâ€“Hochberg false discovery
    rate (FDR)
    '''
    
    data_fit = subject_level_model_evidence
    
    win_model = 'VIEW_INDIPENDENTxCONTEXT'
    indx_models = [i for i in data_fit if 'VIEW_INDIPENDENTxCONTEXT' not in i]
    indx_res = ['p-val','cohen-d','BF10','obs_power','T']
    
    res = pd.DataFrame(index = indx_res)

    #for each time-point

    for model in indx_models:
        res_tt_raw = pg.ttest(data_fit[win_model],data_fit[model])
        res_tt_ind = [res_tt_raw['p-val'][0],
                      res_tt_raw['cohen-d'][0],
                      res_tt_raw['BF10'][0],
                      res_tt_raw['power'][0],
                      res_tt_raw['T'][0]]
        
        clm_name = win_model + '_vs_' + model
        res[clm_name] = res_tt_ind
    #FDR correction
    fdr_dat = res.copy().T.astype(float)
    fdr_dat['pval_FDR'] = pg.multicomp([i for i in fdr_dat['p-val']],
                                          alpha=0.02,
                                          method='fdr_by')[1]
    return fdr_dat


import os
##### location of data
path_data = r'C:\Users\de_hauk\HESSENBOX\apps_tzakiris_rep\data\data_new_12_08_2020\data_raw\csv'
##### loc of class
path_to_class = r'C:\Users\de_hauk\Documents\GitHub\Apps_Tzakiris_2013\Comp_modeling\model'
### loc of ground truth
path_ground_truth = r'C:\Users\de_hauk\HESSENBOX\apps_tzakiris_rep\data\data_new_12_08_2020\data_list\stimuli_list.csv'

### init class with paths
res_abba = power_analyses(path_data,path_ground_truth,path_to_class)

### get pilot_data
res_beh_power1 = res_abba.get_data(True)



########## DOMINIC IDEA 
'''use View-IndependentxContext model fitted with pilot data to generate
artificial data'''

### generate N datasets from each fitted model
res_beh_power2 = res_abba.gen_data(100)
#res_beh_power2 = res_abba.gen_data_rnd_param(100)

#res12345_AA = {}
#for res_beh_power2,curr_data in zip([res_beh_power_a,res_beh_power_b],['res_beh_power_a','res_beh_power_b']):
keys = [i for i in res_beh_power2.keys() if 'A' in i]
rawdata_subsample = []
for key in keys:
    #rawdata_subsample = rawdata_subsample + [(i,j) for i,j in zip(res_beh_power2[key][0],res_beh_power2[key][1])]
    rawdata_subsample = rawdata_subsample + [i for i in res_beh_power2[key]]


res_123456 = {}
### for each sample size *3
for sample_size in tqdm([30]):
    #Ã¤print(sample_size*3)
    res_subsample = {}
    for subsample in range(5):
        print(('subsample',subsample))
        ### take random sample from generated data
        curr_dat_raw = random.sample(rawdata_subsample,sample_size)
        #curr_dat_params_RND = [i[1] for i in curr_dat_raw]
        #curr_dat = [i[0] for i in curr_dat_raw]
        curr_dat = [i for i in curr_dat_raw]
        ### fit data and generate samples        
        res_gen_data1234,n_params =  res_abba.fit_data_seperate(curr_dat,True)
        # fit_param_view_ind_con = [i['subject_level_parameter-estimates']['VIEW_INDIPENDENTxCONTEXT'] for i in res_gen_data1234]
        # comparison_params = pd.DataFrame()
        # for i,j,num in zip(fit_param_view_ind_con,curr_dat_params_RND, range(len(curr_dat_params_RND))):
        #     diff_param = np.absolute(np.array(i)-np.array(j))
        #     comparison_params[num] = diff_param
        # comparison_params['M'] = comparison_params.mean(axis=1)
        # comparison_params['SD'] = comparison_params.std(axis=1)
        # comparison_params.loc[0]
        ###collect model evidence
        subj_lv_ev = pd.DataFrame(index = list(res_gen_data1234[0]['subject_level_model_evidence'].index))
        for indx,data_res in enumerate(res_gen_data1234):
            subj_data = list(data_res['subject_level_model_evidence']['ev'])
            subj_lv_ev[str(indx)] = subj_data
            
        ### compute BIC
        subj_lv_bic_raw = subj_lv_ev.copy().T
        subj_lv_bic_fin = pd.DataFrame()
        group_lv_bic_fin = pd.DataFrame()
        for model in n_params:
            curr_modelev = list(subj_lv_bic_raw[model])
            curr_nparams = n_params[model]
            res_bic_model = []
            #group_lv_bic_fin[model] = [bic(curr_nparams, sample_size, np.sum(curr_modelev))]
            for subj in curr_modelev:
                curr_bic = bic(curr_nparams, sample_size, subj)
                res_bic_model.append(curr_bic)
            subj_lv_bic_fin[model] = res_bic_model
        m_prob = []   
        
        for indx in subj_lv_bic_raw.index:
            subj_BIC_win = np.exp(float(subj_lv_bic_raw.loc[indx]['VIEW_INDIPENDENTxCONTEXT']))
            subj_BIC_cntrl = np.exp(special.logsumexp(np.array(subj_lv_bic_raw.loc[indx])))
            subj_post_prob = np.around(subj_BIC_win/subj_BIC_cntrl,decimals=5)

            m_prob.append(subj_post_prob)
        comparison_params = []
        #perform ttest procedure
        tt_data = subj_lv_bic_fin.copy()
        res_gen_data1234_TT = ttest_procedure(tt_data)
        ###logprob
        
        sum_BIC_win = float(subj_lv_bic_fin['VIEW_INDIPENDENTxCONTEXT'].sum())
        sum_BIC_cntrl = special.logsumexp(np.array(subj_lv_bic_fin.sum(axis=0)))
        post_prob = np.exp(np.around(sum_BIC_win-sum_BIC_cntrl,decimals=5))
        # parameternoise??
        #res_gen_data1234_pmp = np.exp(res_gen_data1234[1][0])/np.exp(special.logsumexp(list(res_gen_data1234[1])))
        res_subsample[str(subsample)] = {'ttest':                   res_gen_data1234_TT,
                                          'res_LL_subj':            subj_lv_ev,
                                          'BIC_group':              group_lv_bic_fin,
                                          'post_model_p_subj':      m_prob,
                                          'post_model_p_subj_M':    np.around(np.mean(m_prob),decimals=5),
                                          'post_model_p_group':     post_prob, 
                                          'param_comp':             comparison_params,
                                          'mean_power':             (np.mean(res_gen_data1234_TT['obs_power']), 
                                                                      res_gen_data1234_TT['obs_power'])}
        
    res_123456[str(sample_size)] = {'res_subsample':            res_subsample,
                                    'raw_post_model_p_subj':    [res_subsample[i]['post_model_p_subj_M']for i in res_subsample],
                                    'M_post_model_p_subj':      np.mean([res_subsample[i]['post_model_p_subj_M']for i in res_subsample]),
                                    'raw_post_model_p_group':    [res_subsample[i]['post_model_p_group']for i in res_subsample],
                                    'M_post_model_p_group':      np.mean([res_subsample[i]['post_model_p_group']for i in res_subsample]),                                    
                                    'raw_sample_power':         [res_subsample[i]['mean_power'][0] for i in res_subsample],
                                    'mean_sample_power':        np.mean([res_subsample[i]['mean_power'][0] for i in res_subsample])}
#res12345_AA[curr_data] = res_123456
# # fitted_data_power = pd.DataFrame(index = ['ttest_arch_pwr', 'post_model_p'])
# # for sample_size in res_123456.keys():
# #     fitted_data_power[sample_size] = [np.around(res_123456[sample_size]['mean_sample_power'],decimals=5),
# #                                       np.around(res_123456[sample_size]['M_post_m_p'],decimals=5)]

# res_1234r = res_123456
# save_path = r'C:\Users\de_hauk\Documents\GitHub\Apps_Tzakiris_2013\Comp_modeling\model\res_power_analysis'
# fitted_data_power.to_csv(save_path+'/' + 'fitted_data_power.csv', sep=',',index = True)

# bbbaaa = pd.read_csv(save_path+'/' + 'fitted_data_power.csv', sep=',')


# ### get req_N for behavioral experiment
# res_gen_data =  res_abba.behavioral_power()

# ########## HAUKE IDEA 
# ### generate synth data
# '''Using Apps&Tsakiris figure to generate artificial samples of answer vectors'''

# ### generate 30 samples for each sample size between (min_sample_size, max_sample_size)
# res_synth_data = res_abba.generate_data_pwr([25,30]) 
# parameter_N = bic_data()
# model_indx = [i for i in parameter_N]
# ### for every sample size
# res_total_power = {}
# for sample_size in tqdm(res_synth_data.keys()):
#     print('curr_sample-size ' + str(sample_size))
#     curr_ss = res_synth_data[sample_size]
#     all_it = []
#     ### convert every subsample of 30 to numpy
#     for iteration in curr_ss.keys():
#         curr_art_sample = curr_ss[iteration]
#         curr_dat = []
#         for j in curr_art_sample.keys():
#             curr_dat.append(curr_art_sample[j].to_numpy())
#         all_it.append(curr_dat)
        
#     ### for every subsample/ iteration of sample
#     res_total = []    
#     for counter, ind_sample in enumerate(all_it):    
#         print('subsample '+str(counter))
#         # fit each generated subsample
#         results123 = Parallel(n_jobs=-1,
#                               verbose=0,
#                               backend='loky')(delayed(fit_data_base)(k) for k in ind_sample)  
        
#         #extract model_fit
#         res_med = pd.DataFrame(index = list(results123[0].index))
#         for counter, value in enumerate(results123):
#             res_med[counter] = value
            
#         ### calc BIV
#         res_bic = pd.DataFrame()
#         for modell in model_indx:
#             raw_LL = res_med.loc[modell].sum()
#             curr_nparams = parameter_N[modell]
#             bic_proc = bic(curr_nparams, sample_size, raw_LL)
#             res_bic[modell] = [bic_proc]
#         #perform ttest-procedure
#         res_tt = ttest_procedure(res_med.T)
#         win_mod_ev = res_med.loc['VIEW_INDIPENDENTxCONTEXT'].sum()
#         cntrl_mod = [res_med.loc[i].sum() for i in res_med.index]
#         post_model_prob = np.exp(win_mod_ev)/np.exp(special.logsumexp(cntrl_mod))
#         res_total.append({'subj_ev': res_med,
#                           'res_ttest':res_tt,
#                           'mean_obs_power' :    res_tt['obs_power'].mean(),
#                           'post_model_prob' :   post_model_prob,
#                           'group_bic': res_bic})
    
#     res_total_power[sample_size] = {'tot_res':          res_total,
#                                     'raw_post_m_p':     [i['post_model_prob'] for i in res_total],
#                                     'M_post_m_p':       np.mean([i['post_model_prob'] for i in res_total]),
#                                     'raw_sample_pwr':   [i['mean_obs_power'] for i in res_total],
#                                     'M_sample_pwr':     np.mean([i['mean_obs_power'] for i in res_total])}
                                    


# synth_data_power = pd.DataFrame(index = ['ttest_arch_pwr', 'post_model_p'])
# for sample_size in res_total_power.keys():
#     synth_data_power[sample_size] = [np.around(res_total_power[sample_size]['M_sample_pwr'] ,decimals=5),
#                                       np.around(res_total_power[sample_size]['M_post_m_p']   ,decimals=5)]

# save_path = r'C:\Users\de_hauk\Documents\GitHub\Apps_Tzakiris_2013\Comp_modeling\model\res_power_analysis'
# synth_data_power.to_csv(save_path+'/' + 'synth_data_power.csv', sep=',',index = True)

# aaabbb = pd.read_csv(save_path+'/' + 'synth_data_power.csv', sep=',')
