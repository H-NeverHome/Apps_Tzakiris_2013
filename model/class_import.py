# -*- coding: utf-8 -*-
"""
Created on Wed Sep 16 09:45:32 2020

@author: de_hauk
"""


def get_data(path):
    import pandas as pd
    ### Get preprocessed and previously stored data 
    data = pd.read_csv(path + r'/final_proc_dat_labjansen.csv')

    #### get unique IDS
    sample_answer_clms = [i for i in data.columns.values.tolist() if 'answer' in i]
    sample_perspective_clms = [i for i in data.columns.values.tolist() if 'perspective' in i]
    
    return {'data':data,
            'view_ind_data':sample_answer_clms,
            'view_dep_data':sample_perspective_clms}


def get_data_2(path_raw_data, ground_truth_file):
    ### Process raw data under path, nothin prestored
    
    import pandas as pd
    import glob
    import numpy as np
    from autoimpute.imputations import MultipleImputer
    data_path = path_raw_data
    #data_path = r'C:\Users\de_hauk\PowerFolders\apps_tzakiris_rep\data\data_new_12_08_2020\data_raw\csv'
    all_files = glob.glob(data_path + "/*.csv")
    
    sample_fullinfo = pd.read_csv(ground_truth_file)
    SAMPLE_fullinfo = sample_fullinfo.drop(columns = ['Unnamed: 0']).copy()  
    
    unq_id = []
    DATA_raw_DF = pd.DataFrame()
    for data in all_files:
        unique_ID = data[-12] + '_' + data[-5] 
        unq_id.append(unique_ID)
        curr_raw_data = pd.read_csv(data,header=None, sep='\t').drop(axis='index', labels = [0,1])[2]
        perspective = []
        answer_correct = [] #1 yes // 0 no
        answer_raw = [] # YES -> 1 // NO -> 0 // Do you remember face?
        for data_point in curr_raw_data:
            if len(data_point) < 5:
                perspective.append(data_point)
            if ('1' in data_point[0])and (len(data_point)>5):
                answer_correct.append(1)
            elif ('0' in data_point[0])and (len(data_point)>5):
                answer_correct.append(0)
            if '[YES]' in data_point:
                answer_raw.append(1)
            elif '[NO]' in data_point:
                answer_raw.append(0)
            elif 'missed' in data_point:
                answer_raw.append(np.nan)
                
    
        DATA_raw_DF[unique_ID + '_perspective'] = perspective
        DATA_raw_DF[unique_ID + '_perf'] = answer_correct
        DATA_raw_DF[unique_ID + '_answer'] = answer_raw
    #raw_dat_unproc = [pd.read_csv(i,header=None, sep='\t').drop(axis='index', labels = [0,1])[2] for i in all_files]
    
    ### determine place and amount of missing value
    missing_dat_raw = pd.DataFrame(DATA_raw_DF.isnull().sum(), columns = ['dat'])
    
    ### filter out clmns with no missing dat
    #missing_dat_overview = missing_dat_raw.loc[missing_dat_raw['dat'] > 0].T
    
    #drop irrelevant columns containig weird stim codings
    miss_perspect = [i for i in DATA_raw_DF if ('perspective' in i) or ('perf' in i)]
    miss_dat = DATA_raw_DF.drop(labels =miss_perspect, axis = 1 ).copy()
    
    # Impute missing dat with binary logistic regress returning only one DF with imputed values
    ### Uses = https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
    imp = MultipleImputer(n=1,strategy= 'binary logistic',return_list=True, imp_kwgs={"binary logistic": {"max_iter": 10000}} )
    impute_res = imp.fit_transform(miss_dat)
    
    # # merge imputed DF with relevant Info i.e. generating list etc
    DATA_imput = impute_res[0][1]
    for i in SAMPLE_fullinfo:
            DATA_imput[i] = sample_fullinfo[i]
    for i in DATA_raw_DF:
        if i not in [i for i in DATA_imput]:
            DATA_imput[i] = DATA_raw_DF[i]
    return {'imputed_data':DATA_imput,
            'raw_data': DATA_raw_DF,
            'stat_ground-truth':SAMPLE_fullinfo,
            'unique_ID': unq_id}

def data_old_sample(path):
    import pandas as pd
    import glob
    import numpy as np
    from autoimpute.imputations import MultipleImputer
    
    data_path = path
    
    
    ############################################### Data #################################################################################
    
    ''' Data Processing in two batches. Second batch containes additional Information in form of an additional row per trial which 
    needs to be deleted'''
    
    ## First Batch
    path_1_batch_raw = data_path + r'\first_batch'
    all_files_1B = glob.glob(path_1_batch_raw + "/*.csv")
    
    ## Second Batch
    path_2_batch_raw = data_path + r'\second_batch'
    all_files_2B = glob.glob(path_2_batch_raw + "/*.csv")
    
    ## Merge Batches so that subj 1-5 are first batch and 6-10 are second batch
    all_files = all_files_1B + all_files_2B
    
    ## Get raw data i.e. trial list generated by previous optimization run
    sample_fullinfo = pd.read_csv(data_path + r'\stimuli_list.csv')
    sample_fullinfo = sample_fullinfo.drop(columns = ['Unnamed: 0']).copy()
    
    ## setup dataframe not containing raw dat
    SAMPLE_onlydat = pd.DataFrame()
    
    raw_dat_unproc = [pd.read_csv(i,sep='\s+', header=None).drop(columns=[0,1,2,5,6,7,8,9]).drop(axis='index', labels = [0]) for i in all_files]
    
    ## For each (subject)-data in directories
    for ID in all_files:
        
        curr_ID = ID[-8:-6] ## get unique ID Number/Subject Number from raw dat names
        curr_file = ID ## get current file-path
        
        ## get actual data into dataframe & drop irrelevant clmns
        df_sample_raw = pd.read_csv(curr_file,sep='\s+', header=None).drop(columns=[0,1,2,5,6,7,8,9]).drop(axis='index', labels = [0])
        
        ## select only relevant rows
        ## ACHTUNG: Length of df = batch 1 < batch 2
        ## length use as a seperator to filter irrelevant rows
        
        if len(df_sample_raw[3]) >= 800: # aka. if data from batch 2
            df_sample = df_sample_raw.loc[(df_sample_raw[4] != '2=switched_1=notswitched') & (df_sample_raw[4] != 'actual_key_press_for_RT')]
        else:
            df_sample = df_sample_raw.loc[(df_sample_raw[4] != 'actual_key_press_for_RT')]
            
        ## Initialize protocoll data
        answer_correct = [] #1==yes//0==no 
        faceID_known = [] #1==yes//0==no
        perspective = []
        
        for i,j in zip(df_sample[3], df_sample[4]):
    
            ### Which stimulus was observed
            if (len(i) == 4) and (i not in ['None','right','left']):
                perspective.append(i)
                
            ### What was the Answer, regardless of correctness??  
            if ('right' in str(i)) and (str(j) == 'left=yes_right=no_None=missed'):
                faceID_known.append(0) # NOT familiar
            elif ('left' in str(i)) and (str(j) == 'left=yes_right=no_None=missed'):
                faceID_known.append(1) # familiar
            elif (str(i) == 'None') and (str(j) == 'left=yes_right=no_None=missed'):
                faceID_known.append(np.nan) # code responding too early and missed responses as np.nan
                
            ### Was the answer correct w.r.t. to the task? // Missing an Answer is also worng
            if (str(i) == '1') and ( 'right' in str(j)):
                answer_correct.append(int(i))
            elif (str(i) == '0') and ( 'wrong' in str(j)):
                answer_correct.append(int(i))
            elif (str(i) == '0') and (str(j) == 'missed'):
                answer_correct.append(int(i))           
          
        
        sample_fullinfo[str(curr_ID) + 'answer'] = pd.Series(faceID_known)    
        sample_fullinfo[str(curr_ID) + 'perf'] = pd.Series(answer_correct)
        sample_fullinfo[str(curr_ID) + 'perspective'] = pd.Series(perspective)
        
        SAMPLE_onlydat[str(curr_ID) + 'answer'] = pd.Series(faceID_known)
        SAMPLE_onlydat[str(curr_ID) + 'perf'] = pd.Series(answer_correct)
        SAMPLE_onlydat[str(curr_ID) + 'perspective'] = pd.Series(perspective)
    
    
    #################################### Impute missing Data ###############################################################
    
    ### determine place and amount of missing value
    missing_dat_raw = pd.DataFrame(sample_fullinfo.isnull().sum(), columns = ['dat'])
    
    ### filter out clmns with no missing dat
    missing_dat_overview = missing_dat_raw.loc[missing_dat_raw['dat'] > 0].T
    
    
    
    #drop irrelevant columns containig weird stim codings
    miss_perspect = [i for i in SAMPLE_onlydat if 'perspective' in i ]
    miss_dat = SAMPLE_onlydat.drop(labels =miss_perspect, axis = 1 ).copy()
    
    # Impute missing dat with binary logistic regress returning only one DF with imputed values
    ### Uses = https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
    imp = MultipleImputer(n=1,strategy= 'binary logistic',return_list=True, imp_kwgs={"binary logistic": {"max_iter": 10000}} )
    impute_res = imp.fit_transform(miss_dat)
    
    # merge imputed DF with relevant Info i.e. generating list etc
    DATA_imput = impute_res[0][1]
    for i in sample_fullinfo:
        if i not in [i for i in sample_fullinfo if ('answer' in i) or ('perf' in i)]:
            DATA_imput[i] = sample_fullinfo[i]
      
    return {'Data_raw_unproc':raw_dat_unproc,
            'Data_raw': SAMPLE_onlydat,
            'DATA_imput': DATA_imput,
            }



def fit_data_noCV(data, lbfgs_epsilon, verbose_tot):
        
    from model_functions_BFGS import VIEW_INDIPENDENTxCONTEXT,VIEW_DEPENDENT,VIEW_DEPENDENTxCONTEXT_DEPENDENT
    from model_functions_BFGS import VIEW_INDEPENDENT, VIEW_INDEPENDENTxVIEW_DEPENDENT
    from model_functions_BFGS import VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT
    
    #folder_path_data = r'C:\Users\de_hauk\PowerFolders\apps_tzakiris_rep\data\processed'
    
    
    
    import pandas as pd
    import numpy as np
    from functools import partial
    from scipy import optimize,special
    np.random.seed(1993)
    ### Get data 
    data = data
    
    #### get unique IDS
    sample_answer_clms = [i for i in data.columns.values.tolist() if 'answer' in i]
    sample_perspective_clms = [i for i in data.columns.values.tolist() if 'perspective' in i]
    
    epsilon_param = lbfgs_epsilon
    x_0_bfgs = 0.5
    params_M1_name = ['alpha', 'sigma', 'beta', 'lamd_a'] 
    params_M2_name = ['alpha', 'beta', 'lamd_a']
    params_M3_name = ['alpha', 'sigma', 'beta', 'lamd_a']
    params_M4_name = ['alpha', 'beta', 'lamd_a']
    params_M5_name = ['alpha_ind', 'alpha_dep', 'beta', 'lamd_a_ind', 'lamd_a_dep']
    params_M6_name = ['alpha_ind', 'alpha_dep', 'sigma', 'beta', 'lamd_a_ind', 'lamd_a_dep']

    models_names = ['VIEW_INDIPENDENTxCONTEXT',
                    'VIEW_DEPENDENT',
                    'VIEW_DEPENDENTxCONTEXT_DEPENDENT',
                    'VIEW_INDEPENDENT',
                    'VIEW_INDEPENDENTxVIEW_DEPENDENT',
                    'VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT']
    
    parameter_est = {'VIEW_INDIPENDENTxCONTEXT': pd.DataFrame(index=params_M1_name),
                    'VIEW_DEPENDENT': pd.DataFrame(index=params_M2_name),
                    'VIEW_DEPENDENTxCONTEXT_DEPENDENT': pd.DataFrame(index=params_M3_name),
                    'VIEW_INDEPENDENT': pd.DataFrame(index=params_M4_name),
                    'VIEW_INDEPENDENTxVIEW_DEPENDENT': pd.DataFrame(index=params_M5_name),
                    'VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT': pd.DataFrame(index=params_M6_name)
                        }
                    
    data_verbose_debug = {}
    res_evidence = pd.DataFrame(index=models_names)
    trialwise_data = {}
    bf_log_group = pd.DataFrame()
    
    for i,j in zip(sample_answer_clms,sample_perspective_clms):
        print(i)
        #func calls & rand starts
    
        # data import
        stim_IDs = data['stim_IDs'] #stimulus IDs of winning model 
        new_ID = data['new_IDs'] #trials where new ID is introduced 
        numb_prev_presentations = data['number_of_prev_presentations_raw '] #number_of_prev_presentations
        stim_IDs_perspective = data[str(j)] #view dependent
        VPN_output = data[str(i)] #VPN answers
        verbose = False
        
        
        ##### Model Optim
        
        
        print('VIEW_INDIPENDENTxCONTEXT')
        data_M1 = [VPN_output, new_ID, numb_prev_presentations, stim_IDs,verbose]

        bounds_M1 = [(0,1),(0,1),(.1,20),(0,2)]
        
        part_func_M1 = partial(VIEW_INDIPENDENTxCONTEXT,data_M1) 
        res1 = optimize.fmin_l_bfgs_b(part_func_M1,
                                      approx_grad = True,
                                      bounds = bounds_M1, 
                                      x0 = [x_0_bfgs for i in range(len(bounds_M1))],
                                      epsilon=epsilon_param)
        parameter_est['VIEW_INDIPENDENTxCONTEXT'][i] = res1[0]
        
        
        print('VIEW_DEPENDENT')
        
        #data = [VPN_output, new_ID, stim_IDs, stim_IDs_perspective, verbose]    
        data_M2 = [VPN_output, new_ID, stim_IDs, stim_IDs_perspective, verbose]

    
        bounds_M2 = [(0,1),(.1,20),(0,2)]
        
        part_func_M2 = partial(VIEW_DEPENDENT,data_M2) 
        res2 = optimize.fmin_l_bfgs_b(part_func_M2,
                                      approx_grad = True,
                                      bounds = bounds_M2, 
                                      x0 = [x_0_bfgs for i in range(len(bounds_M2))],
                                      epsilon=epsilon_param)
        
        parameter_est['VIEW_DEPENDENT'][i] = res2[0]
        
        
        print('VIEW_DEPENDENTxCONTEXT_DEPENDENT')

        #data = [VPN_output, new_ID, stim_IDs, stim_IDs_perspective, verbose]    
        
        data_M3 = [VPN_output, new_ID, stim_IDs, stim_IDs_perspective, verbose]
        bounds_M3 = [(0,1),(0,1),(.1,20),(0,2)]
    
        part_func_M3 = partial(VIEW_DEPENDENTxCONTEXT_DEPENDENT,data_M3) 
        res3 = optimize.fmin_l_bfgs_b(part_func_M3,
                                      approx_grad = True,
                                      bounds = bounds_M3, 
                                      x0 = [x_0_bfgs for i in range(len(bounds_M3))],
                                      epsilon=epsilon_param)
        parameter_est['VIEW_DEPENDENTxCONTEXT_DEPENDENT'][i] = res3[0]
        
        print('VIEW_INDEPENDENT')

        #data = [VPN_output, new_ID, numb_prev_presentations, stim_IDs,verbose]
    
        data_M4 = [VPN_output, new_ID, numb_prev_presentations, stim_IDs,verbose]
        bounds_M4 = [(0,1),(.1,20),(0,2)]
    
        part_func_M4 = partial(VIEW_INDEPENDENT,data_M4) 
        res4 = optimize.fmin_l_bfgs_b(part_func_M4,
                                      approx_grad = True,
                                      bounds = bounds_M4, 
                                      x0 = [x_0_bfgs for i in range(len(bounds_M4))],
                                      epsilon=epsilon_param)
        parameter_est['VIEW_INDEPENDENT'][i] = res4[0]
        
        
        print('VIEW_INDEPENDENTxVIEW_DEPENDENT')
        
        #data = [ VPN_output, new_ID, numb_prev_presentations, stim_IDs, stim_IDs_perspective, verbose]
    
    
        data_M5 = [VPN_output, new_ID, numb_prev_presentations, stim_IDs, stim_IDs_perspective, verbose]
        bounds_M5 = [(0,1),(0,1),(.1,20),(0,2),(0,2)]
    
        part_func_M5 = partial(VIEW_INDEPENDENTxVIEW_DEPENDENT,data_M5) 
        res5 = optimize.fmin_l_bfgs_b(part_func_M5,
                                      approx_grad = True,
                                      bounds = bounds_M5, 
                                      x0 = [x_0_bfgs for i in range(len(bounds_M5))],
                                      epsilon=epsilon_param)
        parameter_est['VIEW_INDEPENDENTxVIEW_DEPENDENT'][i] = res5[0]
        

        print('VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT')
        params_M6_name = ['alpha_ind', 'alpha_dep', 'sigma', 'beta', 'lamd_a_ind', 'lamd_a_dep']
        #data = [VPN_output, new_ID, numb_prev_presentations, stim_IDs, stim_IDs_perspective, verbose]
    
        data_M6 = [VPN_output, new_ID, numb_prev_presentations, stim_IDs, stim_IDs_perspective, verbose]
        bounds_M6 = [(0,1),(0,1),(0,1),(.1,20),(0,2),(0,2)]
    
        part_func_M6 = partial(VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT,data_M6) 
        res6 = optimize.fmin_l_bfgs_b(part_func_M6,
                                      approx_grad = True,
                                      bounds = bounds_M6, 
                                      x0 = [x_0_bfgs for i in range(len(bounds_M6))],
                                      epsilon=epsilon_param)
        
        parameter_est['VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT'][i] = res6[0]
        
        re_evidence_subj = np.array([(-1*i[1]) for i in [res1,res2,res3,res4,res5,res6]])
        res_evidence[i] = re_evidence_subj
        
        ### Subject BF_LOG
        bf_log_subj = re_evidence_subj[0]-special.logsumexp(np.array(re_evidence_subj[1::]))
        bf_log_group[i + '_BF_log'] = [bf_log_subj]
        
        
        # trialwise_dat = {}
        ############################## Verbose == True ###########################
        
        verbose_debug = verbose_tot
        
        data_M_debug = [data_M1, data_M2, data_M3, data_M4, data_M5, data_M6]
        for dat in data_M_debug:
            dat[-1] = True        
        
        if verbose_debug == True:

            data_M1_debug = data_M_debug[0]
            params_m_1 = res1[0]
            m_1 = VIEW_INDIPENDENTxCONTEXT(data_M1_debug, params_m_1)
            
            data_M2_debug = data_M_debug[1]
            params_m_2 = res2[0]
            m_2 = VIEW_DEPENDENT(data_M2_debug, params_m_2)
        
            data_M3_debug = data_M_debug[2]
            params_m_3 = res3[0]
            m_3 = VIEW_DEPENDENTxCONTEXT_DEPENDENT(data_M3_debug, params_m_3)
            
            data_M4_debug = data_M_debug[3]
            params_m_4 = res4[0]
            m_4 = VIEW_INDEPENDENT(data_M4_debug, params_m_4)
        
            data_M5_debug = data_M_debug[4]
            params_m_5 = res5[0]
            m_5 = VIEW_INDEPENDENTxVIEW_DEPENDENT(data_M5_debug, params_m_5)
            
            data_M6_debug = data_M_debug[5]
            params_m_6 = res6[0]
            m_6 = VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT(data_M6_debug, params_m_6)
    
            res_debug = {models_names[0]: m_1,
                         models_names[1]: m_2,
                         models_names[2]: m_3,
                         models_names[3]: m_4,
                         models_names[4]: m_5,
                         models_names[5]: m_6}
            data_verbose_debug[i] = res_debug
            
    #### Get winning model trialwise dat ####
        data_M1_debug = data_M_debug[0]
        params_m_1 = res1[0]
        m_1 = VIEW_INDIPENDENTxCONTEXT(data_M1_debug, params_m_1)        

        trialwise_data[i] = m_1[1]['data_store_1']
    restotal = res_evidence.sum(axis=1)
    cntrl_log = special.logsumexp(np.array(restotal[1::]))
    bf_log = (cntrl_log -(np.array(restotal[0])))
    if verbose_tot==True:
        return (restotal,data_verbose_debug)
    elif verbose_tot==False:
        return {'uncorr_LR_10':np.exp(-1*bf_log),
                'subject_level_model_evidence':res_evidence,
                'group_level_model_evidence':res_evidence.sum(axis=1),
                'subject_level_uncorr_LR': bf_log_group,
                #'total_model-evidence':restotal,
                'used_data': data,
                'subject_level_parameter-estimates':parameter_est,
                'subject_level_trialwise_data_win_model':trialwise_data}
                
def get_behavioral_performance(unimputed_subject_data):
    import pandas as pd
    data = unimputed_subject_data
    ########## Behavioral Performance

    vpn_ids = [i.split('_a')[0] for i in data['raw_data'] if 'answer' in i]
    vpn_answer = [i for i in data['raw_data'] if 'answer' in i]
    vpn_perf = [i.split('_a')[0] for i in data['raw_data'] if '_perf' in i]
    behavioral_perf = pd.DataFrame(index = vpn_ids)
    
    ##### Get %-correct
    perf_dat = [(data['raw_data'][i].copy().sum()/ len(data['raw_data'][i]))*100 for i in vpn_perf]
    behavioral_perf['%-correct'] = perf_dat
    
    ##### Get n_errors
    behavioral_perf['n_errors'] = [len(data['raw_data'][i])-data['raw_data'][i].sum() for i in vpn_perf]
    
    ##### Get Missed
    
    behavioral_perf['missed_answers'] = [data['raw_data'][i].isna().sum() for i in data['raw_data'] if 'answer' in i]
    
    final_dict = {'behavioral_results': behavioral_perf,
                  'used_data':data}
    return final_dict


   
def model_selection_AT():
    import pandas as pd
    import numpy as np
    from scipy import special
    ########## Data Transcribed from paper
    
    ##### Model_selection
    # Formulas from: 
    #   Glover, S., Dixon, P. 
    #   Likelihood ratios: A simple and flexible statistic for empirical psychologists. 
    #   Psychonomic Bulletin & Review 11, 791â€“806 (2004). 
    #   https://doi.org/10.3758/BF03196706
    
    ll_raw = {'View-dependent': [-1590,3],
              'View-dependent_context-dependent': [-1510,4],
              'View-INdependent': [-1480,3],
              'View-independent_context-dependent': [-1425,4],
              'View-independent_View-dependent': [-1475,5],
              'View-independent_View-dependent_context-dependent': [-1430,6],
              }
    ll_dat = pd.DataFrame.from_dict(data=ll_raw)
    ll_dat['indx']=['ll','n_param']
    ll_dat.set_index(keys='indx',inplace=True)
    
    model_win = 'View-independent_context-dependent'
    model_cntrl = [i for i in ll_dat if i != model_win]
    model_all = [i for i in ll_dat]
    
    ll_fin = pd.DataFrame(index = ['LR','LR_corr'])
    n_size = 16
    
    
    for i in model_cntrl:
        # Get Log-Likelihoods
        cntrl_ll = ll_dat[i][0]
        win_ll = ll_dat[model_win][0]
        
        # Get n_params
        win_nparam = ll_dat[model_win][1]
        cntrl_nparam = ll_dat[i][1]
        

    
        # uncorr LR
        lr_var = -2*(cntrl_ll-win_ll)
        # corr LL -> Glover & Dixon, 2004
        lr_corr = lr_var*((np.exp((cntrl_nparam-win_nparam)))**(np.log(n_size)/2))
        
        # POSITIVE VALUES FAVOR WINNING MODEL
        ll_fin[i] = [lr_var]+[lr_corr]
    
    # Compare Multiple Models, lambda_mult     
    lmbda_mult = -2*(special.logsumexp(np.array(ll_dat.T['ll'])-ll_dat[model_win][0]))
    
    #transpose 
    
    
    #get BIC
    bic = []
    for i in model_all:
        raw_ll = ll_dat[i][0]
        nparam = ll_dat[i][1]
        bic_raw = (nparam*np.log(n_size))-(2*raw_ll)
        bic.append(bic_raw)
    
    ll_dat = ll_dat.T
    ll_dat['BIC'] = bic
    return {'AT_model_selection_results_nparams': ll_dat,
            'lambda_mult_model_selection':lmbda_mult,
            'LR_fin': ll_fin}   


def fit_data_noCV_irr_len_data(data, lbfgs_epsilon, verbose_tot):
        
    from model_functions_BFGS import VIEW_INDIPENDENTxCONTEXT,VIEW_DEPENDENT,VIEW_DEPENDENTxCONTEXT_DEPENDENT
    from model_functions_BFGS import VIEW_INDEPENDENT, VIEW_INDEPENDENTxVIEW_DEPENDENT
    from model_functions_BFGS import VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT
    
    #folder_path_data = r'C:\Users\de_hauk\PowerFolders\apps_tzakiris_rep\data\processed'
    
    
    
    import pandas as pd
    import numpy as np
    from functools import partial
    from scipy import optimize,special
    np.random.seed(1993)
    ### Get data 
    data = data
    
    #### get unique IDS
    unique_id = list(data.keys())
    sample_answer_clms = [i+'_answer' for i in unique_id]
    sample_perspective_clms = [i+'_perspective' for i in unique_id]
    
    epsilon_param = lbfgs_epsilon
    x_0_bfgs = 0.5
    params_M1_name = ['alpha', 'sigma', 'beta', 'lamd_a'] 
    params_M2_name = ['alpha', 'beta', 'lamd_a']
    params_M3_name = ['alpha', 'sigma', 'beta', 'lamd_a']
    params_M4_name = ['alpha', 'beta', 'lamd_a']
    params_M5_name = ['alpha_ind', 'alpha_dep', 'beta', 'lamd_a_ind', 'lamd_a_dep']
    params_M6_name = ['alpha_ind', 'alpha_dep', 'sigma', 'beta', 'lamd_a_ind', 'lamd_a_dep']

    models_names = ['VIEW_INDIPENDENTxCONTEXT',
                    'VIEW_DEPENDENT',
                    'VIEW_DEPENDENTxCONTEXT_DEPENDENT',
                    'VIEW_INDEPENDENT',
                    'VIEW_INDEPENDENTxVIEW_DEPENDENT',
                    'VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT']
    
    parameter_est = {'VIEW_INDIPENDENTxCONTEXT': pd.DataFrame(index=params_M1_name),
                    'VIEW_DEPENDENT': pd.DataFrame(index=params_M2_name),
                    'VIEW_DEPENDENTxCONTEXT_DEPENDENT': pd.DataFrame(index=params_M3_name),
                    'VIEW_INDEPENDENT': pd.DataFrame(index=params_M4_name),
                    'VIEW_INDEPENDENTxVIEW_DEPENDENT': pd.DataFrame(index=params_M5_name),
                    'VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT': pd.DataFrame(index=params_M6_name)
                        }
                    
    data_verbose_debug = {}
    res_evidence = pd.DataFrame(index=models_names)
    trialwise_data = {}
    bf_log_group = pd.DataFrame()
    
    for vpn in unique_id:
        print(vpn)
        #func calls & rand starts
        
        curr_data_vpn = data[vpn]
        # data import
        stim_IDs = curr_data_vpn['stim_IDs'] #stimulus IDs of winning model 
        new_ID = curr_data_vpn['new_IDs'] #trials where new ID is introduced 
        numb_prev_presentations = curr_data_vpn['n_prev_pres'] #number_of_prev_presentations
        stim_IDs_perspective = curr_data_vpn[vpn+'_perspective'] #view dependent
        VPN_output = curr_data_vpn[vpn+'_answer'] #VPN answers
        verbose = False
        
        
        ##### Model Optim
        
        i=vpn
        print('VIEW_INDIPENDENTxCONTEXT')
        data_M1 = [VPN_output, new_ID, numb_prev_presentations, stim_IDs,verbose]

        bounds_M1 = [(0,1),(0,1),(.1,20),(0,2)]
        
        part_func_M1 = partial(VIEW_INDIPENDENTxCONTEXT,data_M1) 
        res1 = optimize.fmin_l_bfgs_b(part_func_M1,
                                      approx_grad = True,
                                      bounds = bounds_M1, 
                                      x0 = [x_0_bfgs for i in range(len(bounds_M1))],
                                      epsilon=epsilon_param)
        parameter_est['VIEW_INDIPENDENTxCONTEXT'][i] = res1[0]
        
        
        print('VIEW_DEPENDENT')
        
        #data = [VPN_output, new_ID, stim_IDs, stim_IDs_perspective, verbose]    
        data_M2 = [VPN_output, new_ID, stim_IDs, stim_IDs_perspective, verbose]

    
        bounds_M2 = [(0,1),(.1,20),(0,2)]
        
        part_func_M2 = partial(VIEW_DEPENDENT,data_M2) 
        res2 = optimize.fmin_l_bfgs_b(part_func_M2,
                                      approx_grad = True,
                                      bounds = bounds_M2, 
                                      x0 = [x_0_bfgs for i in range(len(bounds_M2))],
                                      epsilon=epsilon_param)
        
        parameter_est['VIEW_DEPENDENT'][i] = res2[0]
        
        
        print('VIEW_DEPENDENTxCONTEXT_DEPENDENT')

        #data = [VPN_output, new_ID, stim_IDs, stim_IDs_perspective, verbose]    
        
        data_M3 = [VPN_output, new_ID, stim_IDs, stim_IDs_perspective, verbose]
        bounds_M3 = [(0,1),(0,1),(.1,20),(0,2)]
    
        part_func_M3 = partial(VIEW_DEPENDENTxCONTEXT_DEPENDENT,data_M3) 
        res3 = optimize.fmin_l_bfgs_b(part_func_M3,
                                      approx_grad = True,
                                      bounds = bounds_M3, 
                                      x0 = [x_0_bfgs for i in range(len(bounds_M3))],
                                      epsilon=epsilon_param)
        parameter_est['VIEW_DEPENDENTxCONTEXT_DEPENDENT'][i] = res3[0]
        
        print('VIEW_INDEPENDENT')

        #data = [VPN_output, new_ID, numb_prev_presentations, stim_IDs,verbose]
    
        data_M4 = [VPN_output, new_ID, numb_prev_presentations, stim_IDs,verbose]
        bounds_M4 = [(0,1),(.1,20),(0,2)]
    
        part_func_M4 = partial(VIEW_INDEPENDENT,data_M4) 
        res4 = optimize.fmin_l_bfgs_b(part_func_M4,
                                      approx_grad = True,
                                      bounds = bounds_M4, 
                                      x0 = [x_0_bfgs for i in range(len(bounds_M4))],
                                      epsilon=epsilon_param)
        parameter_est['VIEW_INDEPENDENT'][i] = res4[0]
        
        
        print('VIEW_INDEPENDENTxVIEW_DEPENDENT')
        
        #data = [ VPN_output, new_ID, numb_prev_presentations, stim_IDs, stim_IDs_perspective, verbose]
    
    
        data_M5 = [VPN_output, new_ID, numb_prev_presentations, stim_IDs, stim_IDs_perspective, verbose]
        bounds_M5 = [(0,1),(0,1),(.1,20),(0,2),(0,2)]
    
        part_func_M5 = partial(VIEW_INDEPENDENTxVIEW_DEPENDENT,data_M5) 
        res5 = optimize.fmin_l_bfgs_b(part_func_M5,
                                      approx_grad = True,
                                      bounds = bounds_M5, 
                                      x0 = [x_0_bfgs for i in range(len(bounds_M5))],
                                      epsilon=epsilon_param)
        parameter_est['VIEW_INDEPENDENTxVIEW_DEPENDENT'][i] = res5[0]
        

        print('VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT')
        params_M6_name = ['alpha_ind', 'alpha_dep', 'sigma', 'beta', 'lamd_a_ind', 'lamd_a_dep']
        #data = [VPN_output, new_ID, numb_prev_presentations, stim_IDs, stim_IDs_perspective, verbose]
    
        data_M6 = [VPN_output, new_ID, numb_prev_presentations, stim_IDs, stim_IDs_perspective, verbose]
        bounds_M6 = [(0,1),(0,1),(0,1),(.1,20),(0,2),(0,2)]
    
        part_func_M6 = partial(VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT,data_M6) 
        res6 = optimize.fmin_l_bfgs_b(part_func_M6,
                                      approx_grad = True,
                                      bounds = bounds_M6, 
                                      x0 = [x_0_bfgs for i in range(len(bounds_M6))],
                                      epsilon=epsilon_param)
        
        parameter_est['VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT'][i] = res6[0]
        
        re_evidence_subj = np.array([(-1*i[1]) for i in [res1,res2,res3,res4,res5,res6]])
        res_evidence[i] = re_evidence_subj
        
        ### Subject BF_LOG
        bf_log_subj = re_evidence_subj[0]-special.logsumexp(np.array(re_evidence_subj[1::]))
        bf_log_group[i + '_BF_log'] = [bf_log_subj]
        
        
        # trialwise_dat = {}
        ############################## Verbose == True ###########################
        
        verbose_debug = verbose_tot
        
        data_M_debug = [data_M1, data_M2, data_M3, data_M4, data_M5, data_M6]
        for dat in data_M_debug:
            dat[-1] = True        
        
        if verbose_debug == True:

            data_M1_debug = data_M_debug[0]
            params_m_1 = res1[0]
            m_1 = VIEW_INDIPENDENTxCONTEXT(data_M1_debug, params_m_1)
            
            data_M2_debug = data_M_debug[1]
            params_m_2 = res2[0]
            m_2 = VIEW_DEPENDENT(data_M2_debug, params_m_2)
        
            data_M3_debug = data_M_debug[2]
            params_m_3 = res3[0]
            m_3 = VIEW_DEPENDENTxCONTEXT_DEPENDENT(data_M3_debug, params_m_3)
            
            data_M4_debug = data_M_debug[3]
            params_m_4 = res4[0]
            m_4 = VIEW_INDEPENDENT(data_M4_debug, params_m_4)
        
            data_M5_debug = data_M_debug[4]
            params_m_5 = res5[0]
            m_5 = VIEW_INDEPENDENTxVIEW_DEPENDENT(data_M5_debug, params_m_5)
            
            data_M6_debug = data_M_debug[5]
            params_m_6 = res6[0]
            m_6 = VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT(data_M6_debug, params_m_6)
    
            res_debug = {models_names[0]: m_1,
                         models_names[1]: m_2,
                         models_names[2]: m_3,
                         models_names[3]: m_4,
                         models_names[4]: m_5,
                         models_names[5]: m_6}
            data_verbose_debug[i] = res_debug
            
    #### Get winning model trialwise dat ####
        data_M1_debug = data_M_debug[0]
        params_m_1 = res1[0]
        m_1 = VIEW_INDIPENDENTxCONTEXT(data_M1_debug, params_m_1)        

        trialwise_data[i] = m_1[1]['data_store_1']
    restotal = res_evidence.sum(axis=1)
    cntrl_log = special.logsumexp(np.array(restotal[1::]))
    bf_log = (cntrl_log -(np.array(restotal[0])))
    
    results_1 = {'uncorr_LR_10':np.exp(-1*bf_log),
                'subject_level_model_evidence':res_evidence,
                'group_level_model_evidence':res_evidence.sum(axis=1),
                'subject_level_uncorr_LR': bf_log_group,
                #'total_model-evidence':restotal,
                'xxx':data_verbose_debug,
                'used_data': data,
                'subject_level_parameter-estimates':parameter_est,
                'subject_level_trialwise_data_win_model':trialwise_data}
    if verbose_tot==True:
        return (results_1,restotal,data_verbose_debug)
    elif verbose_tot==False:
        return results_1


########## Experimental Below

def VIEW_INDIPENDENTxCONTEXT_t1_t2(data_t1,data_t2, lbfgs_epsilon):
    from model_functions_BFGS import VIEW_INDIPENDENTxCONTEXT
    from tqdm import tqdm
    import pandas as pd
    import numpy as np
    from functools import partial
    from scipy import optimize,special
    np.random.seed(1993)

    #### get unique IDS
    unique_id_t1 = list(data_t1.keys())
    unique_id_t2 = list(data_t2.keys())
    
    sample_answer_clms_t1 = [i+'_answer' for i in unique_id_t1]
    sample_answer_clms_t2 = [i+'_answer' for i in unique_id_t2]
    
    # sample_perspective_clms = [i+'_perspective' for i in unique_id]
    # sample_perspective_clms = [i+'_perspective' for i in unique_id]
    
    epsilon_param = lbfgs_epsilon
    x_0_bfgs = 0.5
    params_M1_name = ['alpha', 'sigma', 'beta', 'lamd_a']     
    models_names = ['VIEW_INDIPENDENTxCONTEXT']
    
    parameter_est = {'VIEW_INDIPENDENTxCONTEXT': pd.DataFrame(index=params_M1_name)}                    
    
    data_verbose_debug = {}
    res_evidence = pd.DataFrame(index=models_names)
    trialwise_data = {}
    bf_log_group = pd.DataFrame()
    
    for vpn_t1,vpn_t2 in zip(unique_id_t1,unique_id_t2):
        print(vpn_t1,vpn_t2)
        #func calls & rand starts
        
        curr_data_vpn_t1 = data_t1[vpn_t1]
        curr_data_vpn_t2 = data_t2[vpn_t2]
        # data import
        stim_ID_t1 = curr_data_vpn_t1['stim_IDs'] #stimulus IDs of winning model
        stim_ID_t2 = curr_data_vpn_t2['stim_IDs'] #stimulus IDs of winning model
        
        new_ID_t1 = curr_data_vpn_t1['new_IDs'] #trials where new ID is introduced 
        new_ID_t2 = curr_data_vpn_t2['new_IDs'] #trials where new ID is introduced 
        
        numb_prev_presentations_t1 = curr_data_vpn_t1['n_prev_pres'] #number_of_prev_presentations
        numb_prev_presentations_t2 = curr_data_vpn_t2['n_prev_pres'] #number_of_prev_presentations

        #stim_IDs_perspective = curr_data_vpn[vpn+'_perspective'] #view dependent
        #stim_IDs_perspective = curr_data_vpn[vpn+'_perspective'] #view dependent

        VPN_output_t1 = curr_data_vpn_t1[vpn_t1+'_answer'] #VPN answers
        VPN_output_t2  = curr_data_vpn_t2[vpn_t2+'_answer'] #VPN answers
        verbose = False
        
        
        def t1_t2(data_M1_t1,data_M1_t2,params):
           res1_t1 = VIEW_INDIPENDENTxCONTEXT(data_M1_t1,params)
           res1_t2 = VIEW_INDIPENDENTxCONTEXT(data_M1_t2,params)                     
           loss_t1 = res1_t1
           loss_t2 = res1_t2
           total_loss = loss_t1 + loss_t2
           return total_loss
       
        
       
        ##### Model Optim
        
        data_M1_t1 = [VPN_output_t1, new_ID_t1, numb_prev_presentations_t1, stim_ID_t1,verbose]
        data_M1_t2 = [VPN_output_t2, new_ID_t2, numb_prev_presentations_t2, stim_ID_t2,verbose]
        part_func_M1_t1_t2 = partial(t1_t2,data_M1_t1,data_M1_t2) 
        bounds_M1= [(0,1),(0,1),(.1,20),(0,2)]
        res1_t1_t2 = optimize.fmin_l_bfgs_b(part_func_M1_t1_t2,
                                      approx_grad = True,
                                      bounds = bounds_M1, 
                                      x0 = [x_0_bfgs for i in range(len(bounds_M1))],
                                      epsilon=epsilon_param)
        #parameter_est['VIEW_INDIPENDENTxCONTEXT_t1'][vpn_t1] = res1_t1_t2[0]      
        
        # print('VIEW_INDIPENDENTxCONTEXT_t1')
        # data_M1_t1 = [VPN_output_t1, new_ID_t1, numb_prev_presentations_t1, stim_ID_t1,verbose]

        # bounds_M1_t1 = [(0,1),(0,1),(.1,20),(0,2)]
        
        # part_func_M1_t1 = partial(VIEW_INDIPENDENTxCONTEXT,data_M1_t1) 
        # res1_t1 = optimize.fmin_l_bfgs_b(part_func_M1_t1,
        #                               approx_grad = True,
        #                               bounds = bounds_M1_t1, 
        #                               x0 = [x_0_bfgs for i in range(len(bounds_M1_t1))],
        #                               epsilon=epsilon_param)
        # parameter_est['VIEW_INDIPENDENTxCONTEXT_t1'][vpn_t1] = res1_t1[0] 
        
                
        # print('VIEW_INDIPENDENTxCONTEXT_t2')
        # data_M1_t2 = [VPN_output_t2, new_ID_t2, numb_prev_presentations_t2, stim_ID_t2,verbose]

        # bounds_M1_t2 = [(0,1),(0,1),(.1,20),(0,2)]
        
        # part_func_M1_t2 = partial(VIEW_INDIPENDENTxCONTEXT,data_M1_t2) 
        # res1_t2 = optimize.fmin_l_bfgs_b(part_func_M1_t2,
        #                               approx_grad = True,
        #                               bounds = bounds_M1_t2, 
        #                               x0 = [x_0_bfgs for i in range(len(bounds_M1_t2))],
        #                               epsilon=epsilon_param)
        parameter_est['VIEW_INDIPENDENTxCONTEXT'][vpn_t1 + vpn_t2] = res1_t1_t2[0] 
        re_evidence_subj = np.array([(-1*i[1]) for i in [res1_t1_t2]])

        
        res_evidence[vpn_t1 + vpn_t2] = re_evidence_subj
    return res_evidence
#concept   
#def model_1_t1_t2_comb(data1,data2,params):

#     loss_t1 = Call m1 func(data1,params)
#     loss_t2 = Call m1 func(data2,params)

#     total_loss = sum(loss_t1,loss_t2)
#     return total_loss

#   part_func_M1 = partial(model_1_t1_t2_comb,data1,data2) 



def fit_data_CV(data, lbfgs_epsilon, verbose_tot):
    # TODO:
        
    from model_functions_BFGS import VIEW_INDIPENDENTxCONTEXT,VIEW_DEPENDENT,VIEW_DEPENDENTxCONTEXT_DEPENDENT
    from model_functions_BFGS import VIEW_INDEPENDENT, VIEW_INDEPENDENTxVIEW_DEPENDENT
    from model_functions_BFGS import VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT
    from model_functions_BFGS import VIEW_INDIPENDENTxCONTEXT_CV,VIEW_DEPENDENT_CV
    from model_functions_BFGS import VIEW_DEPENDENTxCONTEXT_DEPENDENT_CV,VIEW_INDEPENDENT_CV
    #folder_path_data = r'C:\Users\de_hauk\PowerFolders\apps_tzakiris_rep\data\processed'
    
    
    from tqdm import tqdm
    import pandas as pd
    import numpy as np
    from functools import partial
    from scipy import optimize,special
    np.random.seed(1993)
    ### Get data 
    data = data
    
    #### get unique IDS
    unique_id = list(data.keys())
    sample_answer_clms = [i+'_answer' for i in unique_id]
    sample_perspective_clms = [i+'_perspective' for i in unique_id]
    
    epsilon_param = lbfgs_epsilon
    x_0_bfgs = 0.5
    params_M1_name = ['alpha', 'sigma', 'beta', 'lamd_a'] 
    params_M2_name = ['alpha', 'beta', 'lamd_a']
    params_M3_name = ['alpha', 'sigma', 'beta', 'lamd_a']
    params_M4_name = ['alpha', 'beta', 'lamd_a']
    params_M5_name = ['alpha_ind', 'alpha_dep', 'beta', 'lamd_a_ind', 'lamd_a_dep']
    params_M6_name = ['alpha_ind', 'alpha_dep', 'sigma', 'beta', 'lamd_a_ind', 'lamd_a_dep']

    models_names = ['VIEW_INDIPENDENTxCONTEXT',
                    'VIEW_DEPENDENT',
                    'VIEW_DEPENDENTxCONTEXT_DEPENDENT',
                    'VIEW_INDEPENDENT',
                    'VIEW_INDEPENDENTxVIEW_DEPENDENT',
                    'VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT']
    
    parameter_est = {'VIEW_INDIPENDENTxCONTEXT': pd.DataFrame(index=params_M1_name),
                    'VIEW_DEPENDENT': pd.DataFrame(index=params_M2_name),
                    'VIEW_DEPENDENTxCONTEXT_DEPENDENT': pd.DataFrame(index=params_M3_name),
                    'VIEW_INDEPENDENT': pd.DataFrame(index=params_M4_name),
                    'VIEW_INDEPENDENTxVIEW_DEPENDENT': pd.DataFrame(index=params_M5_name),
                    'VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT': pd.DataFrame(index=params_M6_name)
                        }
                    
    data_verbose_debug = {}
    res_evidence = pd.DataFrame(index=models_names)
    trialwise_data = {}
    bf_log_group = pd.DataFrame()
    

    total_results = {}
    
    
    for vpn in tqdm(unique_id, total=len(unique_id)):
        #print(vpn)
        #func calls & rand starts
        vpn_result = {}
        curr_data_vpn = data[vpn]
        # for trial in trial
        cv_score_view_ind_cont = []
        cv_score_view_dep = []
        cv_score_view_dep_cont=[]
        for indx in tqdm(range(curr_data_vpn.shape[0])):
                
            # holdout-data
            holdout_data = curr_data_vpn.copy().loc[indx]
            action = holdout_data[vpn+'_answer']
            # testing data
            test_data = curr_data_vpn.copy().drop(axis=0,index = indx)
            
            # data import
            stim_IDs = test_data['stim_IDs'] #stimulus IDs of winning model 
            new_ID = test_data['new_IDs'] #trials where new ID is introduced 
            numb_prev_presentations = test_data['n_prev_pres'] #number_of_prev_presentations
            stim_IDs_perspective = test_data[vpn+'_perspective'] #view dependent
            VPN_output = test_data[vpn+'_answer'] #VPN answers
            verbose = False
            
            ##### Model Optim
            
############################################################################             
            i=vpn
            #print('VIEW_INDIPENDENTxCONTEXT')
            data_M1 = [VPN_output, new_ID, numb_prev_presentations, stim_IDs,verbose]
    
            bounds_M1 = [(0,1),(0,1),(.1,20),(0,2)]
            
            part_func_M1 = partial(VIEW_INDIPENDENTxCONTEXT,data_M1) 
            res1 = optimize.fmin_l_bfgs_b(part_func_M1,
                                          approx_grad = True,
                                          bounds = bounds_M1, 
                                          x0 = [x_0_bfgs for i in range(len(bounds_M1))],
                                          epsilon=epsilon_param)
            # parameter_est['VIEW_INDIPENDENTxCONTEXT'][i] = res1[0]
            
            # parameter_est['VIEW_INDIPENDENTxCONTEXT'] = res1[0]
            data_M1[-1] = True 
            data_M1_debug = data_M1.copy()
            params_m_1 = res1[0]
            m_1 = VIEW_INDIPENDENTxCONTEXT(data_M1_debug, params_m_1)
            
            init_V_m_1 = 0
            init_C_m_1 = 0
            if indx == 0:
                init_V_m_1 += m_1[1]['init_val']['init_v']
                init_C_m_1 = 0
            else:
                data_cv_score = m_1[1]['data_store_1'].loc[indx-1]
                init_V_m_1 += data_cv_score['history_V']
                init_C_m_1 += data_cv_score['history_C']
    
            #(params,old_vfam,old_cfam,action)
            cv_trial_indeXcontext = VIEW_INDIPENDENTxCONTEXT_CV(params_m_1,init_V_m_1,init_C_m_1,action)
            
            
############################################################################            
            #print('VIEW_DEPENDENT')
            
            #data = [VPN_output, new_ID, stim_IDs, stim_IDs_perspective, verbose]    
            data_M2 = [VPN_output, new_ID, stim_IDs, stim_IDs_perspective, verbose]
            
            data_M2_debug = data_M2.copy()
            data_M2_debug[-1] = True 
            bounds_M2 = [(0,1),(.1,20),(0,2)]
            
            part_func_M2 = partial(VIEW_DEPENDENT,data_M2) 
            res2 = optimize.fmin_l_bfgs_b(part_func_M2,
                                          approx_grad = True,
                                          bounds = bounds_M2, 
                                          x0 = [x_0_bfgs for i in range(len(bounds_M2))],
                                          epsilon=epsilon_param)
            params_m_2 = res2[0]
            parameter_est['VIEW_DEPENDENT'][i] = res2[0]
            #(params, old_fam, action)
            
            m_2 = VIEW_DEPENDENT(data_M2_debug, params_m_2)

            init_V_m_2 = 0
            if indx == 0:
                init_V_m_2 += m_2[1]['init_val']

            else:
                data_cv_score = m_2[1]['history_V_cv'][indx-1]
                init_V_m_2 += data_cv_score

            cv_trial_dep = VIEW_DEPENDENT_CV(params_m_2,init_V_m_2,action)

############################################################################## 

            #print('VIEW_DEPENDENTxCONTEXT_DEPENDENT')
    
            #data = [VPN_output, new_ID, stim_IDs, stim_IDs_perspective, verbose]    
            
            data_M3 = [VPN_output, new_ID, stim_IDs, stim_IDs_perspective, verbose]
            bounds_M3 = [(0,1),(0,1),(.1,20),(0,2)]
        
            part_func_M3 = partial(VIEW_DEPENDENTxCONTEXT_DEPENDENT,data_M3) 
            res3 = optimize.fmin_l_bfgs_b(part_func_M3,
                                          approx_grad = True,
                                          bounds = bounds_M3, 
                                          x0 = [x_0_bfgs for i in range(len(bounds_M3))],
                                          epsilon=epsilon_param)
            parameter_est['VIEW_DEPENDENTxCONTEXT_DEPENDENT'][i] = res3[0]


            data_M3_debug = data_M3.copy()
            data_M3_debug[-1] = True 
            params_m_3 = res3[0]
            m_3 = VIEW_DEPENDENTxCONTEXT_DEPENDENT(data_M3_debug, params_m_3)
            
            init_C_m_3 = 0
            init_V_m_3 = 0
            if indx == 0:
                init_V_m_3 += m_3[1]['suppl']

            else:
                init_C_m_3 = m_3[1]['history_V_dep'][indx]
                init_V_m_3 = m_3[1]['history_C'][indx]


            #(params, old_Vfam_dep, old_c, action)
            cv_trial_dep = VIEW_DEPENDENTxCONTEXT_DEPENDENT_CV(params_m_3,
                                                               init_V_m_3,
                                                               init_C_m_3,
                                                               action)
##############################################################################


            #print('VIEW_INDEPENDENT')
    
            #data = [VPN_output, new_ID, numb_prev_presentations, stim_IDs,verbose]
        
            data_M4 = [VPN_output, new_ID, numb_prev_presentations, stim_IDs,verbose]
            bounds_M4 = [(0,1),(.1,20),(0,2)]
        
            part_func_M4 = partial(VIEW_INDEPENDENT,data_M4) 
            res4 = optimize.fmin_l_bfgs_b(part_func_M4,
                                          approx_grad = True,
                                          bounds = bounds_M4, 
                                          x0 = [x_0_bfgs for i in range(len(bounds_M4))],
                                          epsilon=epsilon_param)
            parameter_est['VIEW_INDEPENDENT'][i] = res4[0]
            
            
                            
            data_M4_debug = data_M4.copy()
            data_M4_debug[-1] = True
            params_m_4 = res4[0]
            m_4 = VIEW_INDEPENDENT(data_M4_debug, params_m_4)

            init_V_m_4 = 0
            if indx == 0:
                init_V_m_4 += m_4[1]['suppl']

            else:
                init_V_m_4 = m_4[1]['history_total'][indx]
            cv_trial_ind = VIEW_INDEPENDENT_CV(params_m_4, init_V_m_4, action)
            
##############################################################################            
            
            cv_score_view_ind_cont.append(cv_trial_indeXcontext)
            cv_score_view_dep.append(cv_trial_dep)
            cv_score_view_dep_cont.append(cv_trial_dep)
        
        df_index = ['VIEW_INDIPENDENTxCONTEXT', 'VIEW_DEPENDENT', 'VIEW_DEPENDENTxCONTEXT_DEPENDENT']
        df_data = [np.sum(cv_score_view_ind_cont),  np.sum(cv_score_view_dep), np.sum(cv_score_view_dep_cont)]
        cv_trial = pd.DataFrame(data = df_data, index=df_index)
        #cv_trial.loc[0],cv_trial.loc[1] = np.sum(np.log(cv_score_view_ind_cont)), np.sum(np.log(cv_score_view_dep))
        total_results[vpn] = cv_trial
        
        
        
        
        
            
           
            
            
            # #print('VIEW_INDEPENDENTxVIEW_DEPENDENT')
            
            # #data = [ VPN_output, new_ID, numb_prev_presentations, stim_IDs, stim_IDs_perspective, verbose]
        
        
            # data_M5 = [VPN_output, new_ID, numb_prev_presentations, stim_IDs, stim_IDs_perspective, verbose]
            # bounds_M5 = [(0,1),(0,1),(.1,20),(0,2),(0,2)]
        
            # part_func_M5 = partial(VIEW_INDEPENDENTxVIEW_DEPENDENT,data_M5) 
            # res5 = optimize.fmin_l_bfgs_b(part_func_M5,
            #                               approx_grad = True,
            #                               bounds = bounds_M5, 
            #                               x0 = [x_0_bfgs for i in range(len(bounds_M5))],
            #                               epsilon=epsilon_param)
            # parameter_est['VIEW_INDEPENDENTxVIEW_DEPENDENT'][i] = res5[0]
            
    
            # #print('VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT')
            # params_M6_name = ['alpha_ind', 'alpha_dep', 'sigma', 'beta', 'lamd_a_ind', 'lamd_a_dep']
            # #data = [VPN_output, new_ID, numb_prev_presentations, stim_IDs, stim_IDs_perspective, verbose]
        
            # data_M6 = [VPN_output, new_ID, numb_prev_presentations, stim_IDs, stim_IDs_perspective, verbose]
            # bounds_M6 = [(0,1),(0,1),(0,1),(.1,20),(0,2),(0,2)]
        
            # part_func_M6 = partial(VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT,data_M6) 
            # res6 = optimize.fmin_l_bfgs_b(part_func_M6,
            #                               approx_grad = True,
            #                               bounds = bounds_M6, 
            #                               x0 = [x_0_bfgs for i in range(len(bounds_M6))],
            #                               epsilon=epsilon_param)
            
            # parameter_est['VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT'][i] = res6[0]
            
            # re_evidence_subj = np.array([(-1*i[1]) for i in [res1,res2,res3,res4,res5,res6]])
            # res_evidence[i] = re_evidence_subj
            
            # ### Subject BF_LOG
            # bf_log_subj = re_evidence_subj[0]-special.logsumexp(np.array(re_evidence_subj[1::]))
            # bf_log_group[i + '_BF_log'] = [bf_log_subj]
            # vpn_result[str(indx)] = re_evidence_subj
        #total_results[vpn] = (np.sum(cv_score_view_ind_cont))
    return total_results


            
        #     # trialwise_dat = {}
        #     ############################## Verbose == True ###########################
            
        #     verbose_debug = verbose_tot
            
        #     data_M_debug = [data_M1, data_M2, data_M3, data_M4, data_M5, data_M6]
        #     for dat in data_M_debug:
        #         dat[-1] = True        
            
        #     if verbose_debug == True:
    

            


        #         data_M5_debug = data_M_debug[4]
        #         params_m_5 = res5[0]
        #         m_5 = VIEW_INDEPENDENTxVIEW_DEPENDENT(data_M5_debug, params_m_5)
                
        #         data_M6_debug = data_M_debug[5]
        #         params_m_6 = res6[0]
        #         m_6 = VIEW_INDEPENDENTxVIEW_DEPENDENTxCONTEXT(data_M6_debug, params_m_6)
        
        #         res_debug = {models_names[0]: m_1,
        #                      models_names[1]: m_2,
        #                      models_names[2]: m_3,
        #                      models_names[3]: m_4,
        #                      models_names[4]: m_5,
        #                      models_names[5]: m_6}
        #         data_verbose_debug[i] = res_debug
                
        # #### Get winning model trialwise dat ####
        #     data_M1_debug = data_M_debug[0]
        #     params_m_1 = res1[0]
        #     m_1 = VIEW_INDIPENDENTxCONTEXT(data_M1_debug, params_m_1)        
    
        #     trialwise_data[i] = m_1[1]['data_store_1']
        # restotal = res_evidence.sum(axis=1)
        # cntrl_log = special.logsumexp(np.array(restotal[1::]))
        # bf_log = (cntrl_log -(np.array(restotal[0])))
        # if verbose_tot==True:
        #     return (restotal,data_verbose_debug)
        # elif verbose_tot==False:
        #     return {'uncorr_LR_10':np.exp(-1*bf_log),
        #             'subject_level_model_evidence':res_evidence,
        #             'group_level_model_evidence':res_evidence.sum(axis=1),
        #             'subject_level_uncorr_LR': bf_log_group,
        #             #'total_model-evidence':restotal,
        #             'used_data': data,
        #             'subject_level_parameter-estimates':parameter_est,
        #             'subject_level_trialwise_data_win_model':trialwise_data}








